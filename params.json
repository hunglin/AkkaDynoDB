{"name":"Akkadynodb","tagline":"Dynamo like distributed database built using Akka Cluster","body":"AkkaDynoDB(Reactive Storage Service)\r\n==========\r\n\r\nDynamo like distributed database built using Akka Cluster\r\n\r\nCluster Usage\r\nFor introduction to the Akka Cluster concepts please see Cluster Specification.\r\n\r\nPreparing Your Project for Clustering\r\nThe Akka cluster is a separate jar file. Make sure that you have the following dependency in your project:\r\n\r\n\"com.typesafe.akka\" %% \"akka-cluster\" % \"2.4-SNAPSHOT\"\r\nA Simple Cluster Example\r\nThe following configuration enables the Cluster extension to be used. It joins the cluster and an actor subscribes to cluster membership events and logs them.\r\n\r\nThe application.conf configuration looks like this:\r\n\r\nakka {\r\n  actor {\r\n    provider = \"akka.cluster.ClusterActorRefProvider\"\r\n  }\r\n  remote {\r\n    log-remote-lifecycle-events = off\r\n    netty.tcp {\r\n      hostname = \"127.0.0.1\"\r\n      port = 0\r\n    }\r\n  }\r\n \r\n  cluster {\r\n    seed-nodes = [\r\n      \"akka.tcp://ClusterSystem@127.0.0.1:2551\",\r\n      \"akka.tcp://ClusterSystem@127.0.0.1:2552\"]\r\n \r\n    auto-down-unreachable-after = 10s\r\n  }\r\n}\r\n \r\n# Disable legacy metrics in akka-cluster.\r\nakka.cluster.metrics.enabled=off\r\n \r\n# Enable metrics extension in akka-cluster-metrics.\r\nakka.extensions=[\"akka.cluster.metrics.ClusterMetricsExtension\"]\r\n \r\n# Sigar native library extract location during tests.\r\n# Note: use per-jvm-instance folder when running multiple jvm on one host. \r\nakka.cluster.metrics.native-library-extract-folder=${user.dir}/target/native\r\nTo enable cluster capabilities in your Akka project you should, at a minimum, add the Remoting settings, but with akka.cluster.ClusterActorRefProvider. The akka.cluster.seed-nodes should normally also be added to your application.conf file.\r\n\r\nThe seed nodes are configured contact points for initial, automatic, join of the cluster.\r\n\r\nNote that if you are going to start the nodes on different machines you need to specify the ip-addresses or host names of the machines in application.conf instead of 127.0.0.1\r\n\r\nAn actor that uses the cluster extension may look like this:\r\n\r\npackage sample.cluster.simple\r\n \r\nimport akka.cluster.Cluster\r\nimport akka.cluster.ClusterEvent._\r\nimport akka.actor.ActorLogging\r\nimport akka.actor.Actor\r\n \r\nclass SimpleClusterListener extends Actor with ActorLogging {\r\n \r\n  val cluster = Cluster(context.system)\r\n \r\n  // subscribe to cluster changes, re-subscribe when restart \r\n  override def preStart(): Unit = {\r\n    //#subscribe\r\n    cluster.subscribe(self, initialStateMode = InitialStateAsEvents,\r\n      classOf[MemberEvent], classOf[UnreachableMember])\r\n    //#subscribe\r\n  }\r\n  override def postStop(): Unit = cluster.unsubscribe(self)\r\n \r\n  def receive = {\r\n    case MemberUp(member) =>\r\n      log.info(\"Member is Up: {}\", member.address)\r\n    case UnreachableMember(member) =>\r\n      log.info(\"Member detected as unreachable: {}\", member)\r\n    case MemberRemoved(member, previousStatus) =>\r\n      log.info(\"Member is Removed: {} after {}\",\r\n        member.address, previousStatus)\r\n    case _: MemberEvent => // ignore\r\n  }\r\n}\r\nThe actor registers itself as subscriber of certain cluster events. It receives events corresponding to the current state of the cluster when the subscription starts and then it receives events for changes that happen in the cluster.\r\n\r\nThe easiest way to run this example yourself is to download Typesafe Activator and open the tutorial named Akka Cluster Samples with Scala. It contains instructions of how to run the SimpleClusterApp.\r\n\r\nJoining to Seed Nodes\r\nYou may decide if joining to the cluster should be done manually or automatically to configured initial contact points, so-called seed nodes. When a new node is started it sends a message to all seed nodes and then sends join command to the one that answers first. If no one of the seed nodes replied (might not be started yet) it retries this procedure until successful or shutdown.\r\n\r\nYou define the seed nodes in the Configuration file (application.conf):\r\n\r\nakka.cluster.seed-nodes = [\r\n  \"akka.tcp://ClusterSystem@host1:2552\",\r\n  \"akka.tcp://ClusterSystem@host2:2552\"]\r\nThis can also be defined as Java system properties when starting the JVM using the following syntax:\r\n\r\n-Dakka.cluster.seed-nodes.0=akka.tcp://ClusterSystem@host1:2552\r\n-Dakka.cluster.seed-nodes.1=akka.tcp://ClusterSystem@host2:2552\r\nThe seed nodes can be started in any order and it is not necessary to have all seed nodes running, but the node configured as the first element in the seed-nodes configuration list must be started when initially starting a cluster, otherwise the other seed-nodes will not become initialized and no other node can join the cluster. The reason for the special first seed node is to avoid forming separated islands when starting from an empty cluster. It is quickest to start all configured seed nodes at the same time (order doesn't matter), otherwise it can take up to the configured seed-node-timeout until the nodes can join.\r\n\r\nOnce more than two seed nodes have been started it is no problem to shut down the first seed node. If the first seed node is restarted, it will first try to join the other seed nodes in the existing cluster.\r\n\r\nIf you don't configure seed nodes you need to join the cluster programmatically or manually.\r\n\r\nManual joining can be performed by using ref:cluster_jmx_scala or Command Line Management. Joining programatically can be performed with Cluster(system).join.\r\n\r\nYou can join to any node in the cluster. It does not have to be configured as a seed node. Note that you can only join to an existing cluster member, which means that for bootstrapping some node must join itself.\r\n\r\nYou may also use Cluster(system).joinSeedNodes to join programmatically, which is attractive when dynamically discovering other nodes at startup by using some external tool or API. When using joinSeedNodes you should not include the node itself except for the node that is supposed to be the first seed node, and that should be placed first in parameter to joinSeedNodes.\r\n\r\nUnsuccessful join attempts are automatically retried after the time period defined in configuration property retry-unsuccessful-join-after. When using seed-nodes this means that a new seed node is picked. When joining manually or programatically this means that the last join request is retried. Retries can be disabled by setting the property to off.\r\n\r\nAn actor system can only join a cluster once. Additional attempts will be ignored. When it has successfully joined it must be restarted to be able to join another cluster or to join the same cluster again. It can use the same host name and port after the restart, but it must have been removed from the cluster before the join request is accepted.\r\n\r\nAutomatic vs. Manual Downing\r\nWhen a member is considered by the failure detector to be unreachable the leader is not allowed to perform its duties, such as changing status of new joining members to 'Up'. The node must first become reachable again, or the status of the unreachable member must be changed to 'Down'. Changing status to 'Down' can be performed automatically or manually. By default it must be done manually, using JMX or Command Line Management.\r\n\r\nIt can also be performed programatically with Cluster(system).down(address).\r\n\r\nYou can enable automatic downing with configuration:\r\n\r\nakka.cluster.auto-down-unreachable-after = 120s\r\nThis means that the cluster leader member will change the unreachable node status to down automatically after the configured time of unreachability.\r\n\r\nBe aware of that using auto-down implies that two separate clusters will automatically be formed in case of network partition. That might be desired by some applications but not by others.\r\n\r\nNote\r\nIf you have auto-down enabled and the failure detector triggers, you can over time end up with a lot of single node clusters if you don't put measures in place to shut down nodes that have become unreachable. This follows from the fact that the unreachable node will likely see the rest of the cluster as unreachable, become its own leader and form its own cluster.\r\n\r\nLeaving\r\nThere are two ways to remove a member from the cluster.\r\n\r\nYou can just stop the actor system (or the JVM process). It will be detected as unreachable and removed after the automatic or manual downing as described above.\r\n\r\nA more graceful exit can be performed if you tell the cluster that a node shall leave. This can be performed using JMX or Command Line Management. It can also be performed programatically with Cluster(system).leave(address).\r\n\r\nNote that this command can be issued to any member in the cluster, not necessarily the one that is leaving. The cluster extension, but not the actor system or JVM, of the leaving member will be shutdown after the leader has changed status of the member to Exiting. Thereafter the member will be removed from the cluster. Normally this is handled automatically, but in case of network failures during this process it might still be necessary to set the nodeâ€™s status to Down in order to complete the removal.\r\n\r\nSubscribe to Cluster Events\r\nYou can subscribe to change notifications of the cluster membership by using Cluster(system).subscribe.\r\n\r\ncluster.subscribe(self, classOf[MemberEvent], classOf[UnreachableMember])\r\nA snapshot of the full state, akka.cluster.ClusterEvent.CurrentClusterState, is sent to the subscriber as the first message, followed by events for incremental updates.\r\n\r\nNote that you may receive an empty CurrentClusterState, containing no members, if you start the subscription before the initial join procedure has completed. This is expected behavior. When the node has been accepted in the cluster you will receive MemberUp for that node, and other nodes.\r\n\r\nIf you find it inconvenient to handle the CurrentClusterState you can use ClusterEvent.InitialStateAsEvents as parameter to subscribe. That means that instead of receiving CurrentClusterState as the first message you will receive the events corresponding to the current state to mimic what you would have seen if you were listening to the events when they occurred in the past. Note that those initial events only correspond to the current state and it is not the full history of all changes that actually has occurred in the cluster.\r\n\r\ncluster.subscribe(self, initialStateMode = InitialStateAsEvents,\r\n  classOf[MemberEvent], classOf[UnreachableMember])\r\nThe events to track the life-cycle of members are:\r\n\r\nClusterEvent.MemberUp - A new member has joined the cluster and its status has been changed to Up.\r\nClusterEvent.MemberExited - A member is leaving the cluster and its status has been changed to Exiting Note that the node might already have been shutdown when this event is published on another node.\r\nClusterEvent.MemberRemoved - Member completely removed from the cluster.\r\nClusterEvent.UnreachableMember - A member is considered as unreachable, detected by the failure detector of at least one other node.\r\nClusterEvent.ReachableMember - A member is considered as reachable again, after having been unreachable. All nodes that previously detected it as unreachable has detected it as reachable again.\r\nThere are more types of change events, consult the API documentation of classes that extends akka.cluster.ClusterEvent.ClusterDomainEvent for details about the events.\r\n\r\nInstead of subscribing to cluster events it can sometimes be convenient to only get the full membership state with Cluster(system).state. Note that this state is not necessarily in sync with the events published to a cluster subscription.\r\n\r\nWorker Dial-in Example\r\nLet's take a look at an example that illustrates how workers, here named backend, can detect and register to new master nodes, here named frontend.\r\n\r\nThe example application provides a service to transform text. When some text is sent to one of the frontend services, it will be delegated to one of the backend workers, which performs the transformation job, and sends the result back to the original client. New backend nodes, as well as new frontend nodes, can be added or removed to the cluster dynamically.\r\n\r\nMessages:\r\n\r\nfinal case class TransformationJob(text: String)\r\nfinal case class TransformationResult(text: String)\r\nfinal case class JobFailed(reason: String, job: TransformationJob)\r\ncase object BackendRegistration\r\nThe backend worker that performs the transformation job:\r\n\r\nclass TransformationBackend extends Actor {\r\n \r\n  val cluster = Cluster(context.system)\r\n \r\n  // subscribe to cluster changes, MemberUp\r\n  // re-subscribe when restart\r\n  override def preStart(): Unit = cluster.subscribe(self, classOf[MemberUp])\r\n  override def postStop(): Unit = cluster.unsubscribe(self)\r\n \r\n  def receive = {\r\n    case TransformationJob(text) => sender() ! TransformationResult(text.toUpperCase)\r\n    case state: CurrentClusterState =>\r\n      state.members.filter(_.status == MemberStatus.Up) foreach register\r\n    case MemberUp(m) => register(m)\r\n  }\r\n \r\n  def register(member: Member): Unit =\r\n    if (member.hasRole(\"frontend\"))\r\n      context.actorSelection(RootActorPath(member.address) / \"user\" / \"frontend\") !\r\n        BackendRegistration\r\n}\r\nNote that the TransformationBackend actor subscribes to cluster events to detect new, potential, frontend nodes, and send them a registration message so that they know that they can use the backend worker.\r\n\r\nThe frontend that receives user jobs and delegates to one of the registered backend workers:\r\n\r\nclass TransformationFrontend extends Actor {\r\n \r\n  var backends = IndexedSeq.empty[ActorRef]\r\n  var jobCounter = 0\r\n \r\n  def receive = {\r\n    case job: TransformationJob if backends.isEmpty =>\r\n      sender() ! JobFailed(\"Service unavailable, try again later\", job)\r\n \r\n    case job: TransformationJob =>\r\n      jobCounter += 1\r\n      backends(jobCounter % backends.size) forward job\r\n \r\n    case BackendRegistration if !backends.contains(sender()) =>\r\n      context watch sender()\r\n      backends = backends :+ sender()\r\n \r\n    case Terminated(a) =>\r\n      backends = backends.filterNot(_ == a)\r\n  }\r\n}\r\nNote that the TransformationFrontend actor watch the registered backend to be able to remove it from its list of available backend workers. Death watch uses the cluster failure detector for nodes in the cluster, i.e. it detects network failures and JVM crashes, in addition to graceful termination of watched actor. Death watch generates the Terminated message to the watching actor when the unreachable cluster node has been downed and removed.\r\n\r\nThe Typesafe Activator tutorial named Akka Cluster Samples with Scala. contains the full source code and instructions of how to run the Worker Dial-in Example.\r\n\r\nNode Roles\r\nNot all nodes of a cluster need to perform the same function: there might be one sub-set which runs the web front-end, one which runs the data access layer and one for the number-crunching. Deployment of actorsâ€”for example by cluster-aware routersâ€”can take node roles into account to achieve this distribution of responsibilities.\r\n\r\nThe roles of a node is defined in the configuration property named akka.cluster.roles and it is typically defined in the start script as a system property or environment variable.\r\n\r\nThe roles of the nodes is part of the membership information in MemberEvent that you can subscribe to.\r\n\r\nHow To Startup when Cluster Size Reached\r\nA common use case is to start actors after the cluster has been initialized, members have joined, and the cluster has reached a certain size.\r\n\r\nWith a configuration option you can define required number of members before the leader changes member status of 'Joining' members to 'Up'.\r\n\r\nakka.cluster.min-nr-of-members = 3\r\nIn a similar way you can define required number of members of a certain role before the leader changes member status of 'Joining' members to 'Up'.\r\n\r\nakka.cluster.role {\r\n  frontend.min-nr-of-members = 1\r\n  backend.min-nr-of-members = 2\r\n}\r\nYou can start the actors in a registerOnMemberUp callback, which will be invoked when the current member status is changed tp 'Up', i.e. the cluster has at least the defined number of members.\r\n\r\nCluster(system) registerOnMemberUp {\r\n  system.actorOf(Props(classOf[FactorialFrontend], upToN, true),\r\n    name = \"factorialFrontend\")\r\n}\r\nThis callback can be used for other things than starting actors.\r\n\r\nCluster Singleton\r\nFor some use cases it is convenient and sometimes also mandatory to ensure that you have exactly one actor of a certain type running somewhere in the cluster.\r\n\r\nThis can be implemented by subscribing to member events, but there are several corner cases to consider. Therefore, this specific use case is made easily accessible by the Cluster Singleton in the contrib module.\r\n\r\nCluster Sharding\r\nDistributes actors across several nodes in the cluster and supports interaction with the actors using their logical identifier, but without having to care about their physical location in the cluster.\r\n\r\nSee Cluster Sharding in the contrib module.\r\n\r\nDistributed Publish Subscribe\r\nPublish-subscribe messaging between actors in the cluster, and point-to-point messaging using the logical path of the actors, i.e. the sender does not have to know on which node the destination actor is running.\r\n\r\nSee Distributed Publish Subscribe in Cluster in the contrib module.\r\n\r\nCluster Client\r\nCommunication from an actor system that is not part of the cluster to actors running somewhere in the cluster. The client does not have to know on which node the destination actor is running.\r\n\r\nSee Cluster Client in the contrib module.\r\n\r\nFailure Detector\r\nIn a cluster each node is monitored by a few (default maximum 5) other nodes, and when any of these detects the node as unreachable that information will spread to the rest of the cluster through the gossip. In other words, only one node needs to mark a node unreachable to have the rest of the cluster mark that node unreachable.\r\n\r\nThe failure detector will also detect if the node becomes reachable again. When all nodes that monitored the unreachable node detects it as reachable again the cluster, after gossip dissemination, will consider it as reachable.\r\n\r\nIf system messages cannot be delivered to a node it will be quarantined and then it cannot come back from unreachable. This can happen if the there are too many unacknowledged system messages (e.g. watch, Terminated, remote actor deployment, failures of actors supervised by remote parent). Then the node needs to be moved to the down or removed states and the actor system must be restarted before it can join the cluster again.\r\n\r\nThe nodes in the cluster monitor each other by sending heartbeats to detect if a node is unreachable from the rest of the cluster. The heartbeat arrival times is interpreted by an implementation of The Phi Accrual Failure Detector.\r\n\r\nThe suspicion level of failure is given by a value called phi. The basic idea of the phi failure detector is to express the value of phi on a scale that is dynamically adjusted to reflect current network conditions.\r\n\r\nThe value of phi is calculated as:\r\n\r\nphi = -log10(1 - F(timeSinceLastHeartbeat))\r\nwhere F is the cumulative distribution function of a normal distribution with mean and standard deviation estimated from historical heartbeat inter-arrival times.\r\n\r\nIn the Configuration you can adjust the akka.cluster.failure-detector.threshold to define when a phi value is considered to be a failure.\r\n\r\nA low threshold is prone to generate many false positives but ensures a quick detection in the event of a real crash. Conversely, a high threshold generates fewer mistakes but needs more time to detect actual crashes. The default threshold is 8 and is appropriate for most situations. However in cloud environments, such as Amazon EC2, the value could be increased to 12 in order to account for network issues that sometimes occur on such platforms.\r\n\r\nThe following chart illustrates how phi increase with increasing time since the previous heartbeat.\r\n\r\n../_images/phi11.png\r\nPhi is calculated from the mean and standard deviation of historical inter arrival times. The previous chart is an example for standard deviation of 200 ms. If the heartbeats arrive with less deviation the curve becomes steeper, i.e. it is possible to determine failure more quickly. The curve looks like this for a standard deviation of 100 ms.\r\n\r\n../_images/phi21.png\r\nTo be able to survive sudden abnormalities, such as garbage collection pauses and transient network failures the failure detector is configured with a margin, akka.cluster.failure-detector.acceptable-heartbeat-pause. You may want to adjust the Configuration of this depending on you environment. This is how the curve looks like for acceptable-heartbeat-pause configured to 3 seconds.\r\n\r\n../_images/phi31.png\r\nDeath watch uses the cluster failure detector for nodes in the cluster, i.e. it detects network failures and JVM crashes, in addition to graceful termination of watched actor. Death watch generates the Terminated message to the watching actor when the unreachable cluster node has been downed and removed.\r\n\r\nIf you encounter suspicious false positives when the system is under load you should define a separate dispatcher for the cluster actors as described in Cluster Dispatcher.\r\n\r\nCluster Aware Routers\r\nAll routers can be made aware of member nodes in the cluster, i.e. deploying new routees or looking up routees on nodes in the cluster. When a node becomes unreachable or leaves the cluster the routees of that node are automatically unregistered from the router. When new nodes join the cluster, additional routees are added to the router, according to the configuration. Routees are also added when a node becomes reachable again, after having been unreachable.\r\n\r\nThere are two distinct types of routers.\r\n\r\nGroup - router that sends messages to the specified path using actor selection The routees can be shared among routers running on different nodes in the cluster. One example of a use case for this type of router is a service running on some backend nodes in the cluster and used by routers running on front-end nodes in the cluster.\r\nPool - router that creates routees as child actors and deploys them on remote nodes. Each router will have its own routee instances. For example, if you start a router on 3 nodes in a 10-node cluster, you will have 30 routees in total if the router is configured to use one instance per node. The routees created by the different routers will not be shared among the routers. One example of a use case for this type of router is a single master that coordinates jobs and delegates the actual work to routees running on other nodes in the cluster.\r\nRouter with Group of Routees\r\nWhen using a Group you must start the routee actors on the cluster member nodes. That is not done by the router. The configuration for a group looks like this:\r\n\r\nakka.actor.deployment {\r\n  /statsService/workerRouter {\r\n      router = consistent-hashing-group\r\n      nr-of-instances = 100\r\n      routees.paths = [\"/user/statsWorker\"]\r\n      cluster {\r\n        enabled = on\r\n        allow-local-routees = on\r\n        use-role = compute\r\n      }\r\n    }\r\n}\r\nNote\r\nThe routee actors should be started as early as possible when starting the actor system, because the router will try to use them as soon as the member status is changed to 'Up'.\r\n\r\nThe relative actor paths defined in routees.paths are used as for selecting the actors to which the messages will be forwarded to by the router. Messages will be forwarded to the routees using ActorSelection, so the same delivery semantics should be expected. It is possible to limit the lookup of routees to member nodes tagged with a certain role by specifying use-role.\r\n\r\nnr-of-instances defines total number of routees in the cluster. Setting nr-of-instances to a high value will result in new routees added to the router when nodes join the cluster.\r\n\r\nThe same type of router could also have been defined in code:\r\n\r\nimport akka.cluster.routing.ClusterRouterGroup\r\nimport akka.cluster.routing.ClusterRouterGroupSettings\r\nimport akka.routing.ConsistentHashingGroup\r\n \r\nval workerRouter = context.actorOf(\r\n  ClusterRouterGroup(ConsistentHashingGroup(Nil), ClusterRouterGroupSettings(\r\n    totalInstances = 100, routeesPaths = List(\"/user/statsWorker\"),\r\n    allowLocalRoutees = true, useRole = Some(\"compute\"))).props(),\r\n  name = \"workerRouter2\")\r\nSee Configuration section for further descriptions of the settings.\r\n\r\nRouter Example with Group of Routees\r\nLet's take a look at how to use a cluster aware router with a group of routees, i.e. router sending to the paths of the routees.\r\n\r\nThe example application provides a service to calculate statistics for a text. When some text is sent to the service it splits it into words, and delegates the task to count number of characters in each word to a separate worker, a routee of a router. The character count for each word is sent back to an aggregator that calculates the average number of characters per word when all results have been collected.\r\n\r\nMessages:\r\n\r\nfinal case class StatsJob(text: String)\r\nfinal case class StatsResult(meanWordLength: Double)\r\nfinal case class JobFailed(reason: String)\r\nThe worker that counts number of characters in each word:\r\n\r\nclass StatsWorker extends Actor {\r\n  var cache = Map.empty[String, Int]\r\n  def receive = {\r\n    case word: String =>\r\n      val length = cache.get(word) match {\r\n        case Some(x) => x\r\n        case None =>\r\n          val x = word.length\r\n          cache += (word -> x)\r\n          x\r\n      }\r\n \r\n      sender() ! length\r\n  }\r\n}\r\nThe service that receives text from users and splits it up into words, delegates to workers and aggregates:\r\n\r\nclass StatsService extends Actor {\r\n  // This router is used both with lookup and deploy of routees. If you\r\n  // have a router with only lookup of routees you can use Props.empty\r\n  // instead of Props[StatsWorker.class].\r\n  val workerRouter = context.actorOf(FromConfig.props(Props[StatsWorker]),\r\n    name = \"workerRouter\")\r\n \r\n  def receive = {\r\n    case StatsJob(text) if text != \"\" =>\r\n      val words = text.split(\" \")\r\n      val replyTo = sender() // important to not close over sender()\r\n      // create actor that collects replies from workers\r\n      val aggregator = context.actorOf(Props(\r\n        classOf[StatsAggregator], words.size, replyTo))\r\n      words foreach { word =>\r\n        workerRouter.tell(\r\n          ConsistentHashableEnvelope(word, word), aggregator)\r\n      }\r\n  }\r\n}\r\n \r\nclass StatsAggregator(expectedResults: Int, replyTo: ActorRef) extends Actor {\r\n  var results = IndexedSeq.empty[Int]\r\n  context.setReceiveTimeout(3.seconds)\r\n \r\n  def receive = {\r\n    case wordCount: Int =>\r\n      results = results :+ wordCount\r\n      if (results.size == expectedResults) {\r\n        val meanWordLength = results.sum.toDouble / results.size\r\n        replyTo ! StatsResult(meanWordLength)\r\n        context.stop(self)\r\n      }\r\n    case ReceiveTimeout =>\r\n      replyTo ! JobFailed(\"Service unavailable, try again later\")\r\n      context.stop(self)\r\n  }\r\n}\r\nNote, nothing cluster specific so far, just plain actors.\r\n\r\nAll nodes start StatsService and StatsWorker actors. Remember, routees are the workers in this case. The router is configured with routees.paths:\r\n\r\nakka.actor.deployment {\r\n  /statsService/workerRouter {\r\n    router = consistent-hashing-group\r\n    nr-of-instances = 100\r\n    routees.paths = [\"/user/statsWorker\"]\r\n    cluster {\r\n      enabled = on\r\n      allow-local-routees = on\r\n      use-role = compute\r\n    }\r\n  }\r\n}\r\nThis means that user requests can be sent to StatsService on any node and it will use StatsWorker on all nodes.\r\n\r\nThe Typesafe Activator tutorial named Akka Cluster Samples with Scala. contains the full source code and instructions of how to run the Router Example with Group of Routees.\r\n\r\nRouter with Pool of Remote Deployed Routees\r\nWhen using a Pool with routees created and deployed on the cluster member nodes the configuration for a router looks like this:\r\n\r\nakka.actor.deployment {\r\n  /singleton/statsService/workerRouter {\r\n      router = consistent-hashing-pool\r\n      nr-of-instances = 100\r\n      cluster {\r\n        enabled = on\r\n        max-nr-of-instances-per-node = 3\r\n        allow-local-routees = on\r\n        use-role = compute\r\n      }\r\n    }\r\n}\r\nIt is possible to limit the deployment of routees to member nodes tagged with a certain role by specifying use-role.\r\n\r\nnr-of-instances defines total number of routees in the cluster, but the number of routees per node, max-nr-of-instances-per-node, will not be exceeded. Setting nr-of-instances to a high value will result in creating and deploying additional routees when new nodes join the cluster.\r\n\r\nThe same type of router could also have been defined in code:\r\n\r\nimport akka.cluster.routing.ClusterRouterPool\r\nimport akka.cluster.routing.ClusterRouterPoolSettings\r\nimport akka.routing.ConsistentHashingPool\r\n \r\nval workerRouter = context.actorOf(\r\n  ClusterRouterPool(ConsistentHashingPool(0), ClusterRouterPoolSettings(\r\n    totalInstances = 100, maxInstancesPerNode = 3,\r\n    allowLocalRoutees = false, useRole = None)).props(Props[StatsWorker]),\r\n  name = \"workerRouter3\")\r\nSee Configuration section for further descriptions of the settings.\r\n\r\nRouter Example with Pool of Remote Deployed Routees\r\nLet's take a look at how to use a cluster aware router on single master node that creates and deploys workers. To keep track of a single master we use the Cluster Singleton in the contrib module. The ClusterSingletonManager is started on each node.\r\n\r\nsystem.actorOf(ClusterSingletonManager.props(\r\n  singletonProps = Props[StatsService], singletonName = \"statsService\",\r\n  terminationMessage = PoisonPill, role = Some(\"compute\")),\r\n  name = \"singleton\")\r\nWe also need an actor on each node that keeps track of where current single master exists and delegates jobs to the StatsService. That is provided by the ClusterSingletonProxy.\r\n\r\nsystem.actorOf(ClusterSingletonProxy.props(singletonPath = \"/user/singleton/statsService\",\r\n  role = Some(\"compute\")), name = \"statsServiceProxy\")\r\nThe ClusterSingletonProxy receives text from users and delegates to the current StatsService, the single master. It listens to cluster events to lookup the StatsService on the oldest node.\r\n\r\nAll nodes start ClusterSingletonProxy and the ClusterSingletonManager. The router is now configured like this:\r\n\r\nakka.actor.deployment {\r\n  /singleton/statsService/workerRouter {\r\n    router = consistent-hashing-pool\r\n    nr-of-instances = 100\r\n    cluster {\r\n      enabled = on\r\n      max-nr-of-instances-per-node = 3\r\n      allow-local-routees = on\r\n      use-role = compute\r\n    }\r\n  }\r\n}\r\nThe Typesafe Activator tutorial named Akka Cluster Samples with Scala. contains the full source code and instructions of how to run the Router Example with Pool of Remote Deployed Routees.\r\n\r\nCluster Metrics\r\nThe member nodes of the cluster can collect system health metrics and publish that to other cluster nodes and to the registered subscribers on the system event bus with the help of Cluster Metrics Extension.\r\n\r\nHow to Test\r\nMulti Node Testing is useful for testing cluster applications.\r\n\r\nSet up your project according to the instructions in Multi Node Testing and Multi JVM Testing, i.e. add the sbt-multi-jvm plugin and the dependency to akka-multi-node-testkit.\r\n\r\nFirst, as described in Multi Node Testing, we need some scaffolding to configure the MultiNodeSpec. Define the participating roles and their Configuration in an object extending MultiNodeConfig:\r\n\r\nimport akka.remote.testkit.MultiNodeConfig\r\nimport com.typesafe.config.ConfigFactory\r\n \r\nobject StatsSampleSpecConfig extends MultiNodeConfig {\r\n  // register the named roles (nodes) of the test\r\n  val first = role(\"first\")\r\n  val second = role(\"second\")\r\n  val third = role(\"thrid\")\r\n \r\n  def nodeList = Seq(first, second, third)\r\n \r\n  // Extract individual sigar library for every node.\r\n  nodeList foreach { role â‡’\r\n    nodeConfig(role) {\r\n      ConfigFactory.parseString(s\"\"\"\r\n      # Disable legacy metrics in akka-cluster.\r\n      akka.cluster.metrics.enabled=off\r\n      # Enable metrics extension in akka-cluster-metrics.\r\n      akka.extensions=[\"akka.cluster.metrics.ClusterMetricsExtension\"]\r\n      # Sigar native library extract location during tests.\r\n      akka.cluster.metrics.native-library-extract-folder=target/native/${role.name}\r\n      \"\"\")\r\n    }\r\n  }\r\n \r\n  // this configuration will be used for all nodes\r\n  // note that no fixed host names and ports are used\r\n  commonConfig(ConfigFactory.parseString(\"\"\"\r\n    akka.actor.provider = \"akka.cluster.ClusterActorRefProvider\"\r\n    akka.remote.log-remote-lifecycle-events = off\r\n    akka.cluster.roles = [compute]\r\n     // router lookup config ...\r\n    \"\"\"))\r\n \r\n}\r\nDefine one concrete test class for each role/node. These will be instantiated on the different nodes (JVMs). They can be implemented differently, but often they are the same and extend an abstract test class, as illustrated here.\r\n\r\n// need one concrete test class per node\r\nclass StatsSampleSpecMultiJvmNode1 extends StatsSampleSpec\r\nclass StatsSampleSpecMultiJvmNode2 extends StatsSampleSpec\r\nclass StatsSampleSpecMultiJvmNode3 extends StatsSampleSpec\r\nNote the naming convention of these classes. The name of the classes must end with MultiJvmNode1, MultiJvmNode2 and so on. It is possible to define another suffix to be used by the sbt-multi-jvm, but the default should be fine in most cases.\r\n\r\nThen the abstract MultiNodeSpec, which takes the MultiNodeConfig as constructor parameter.\r\n\r\nimport org.scalatest.BeforeAndAfterAll\r\nimport org.scalatest.WordSpecLike\r\nimport org.scalatest.Matchers\r\nimport akka.remote.testkit.MultiNodeSpec\r\nimport akka.testkit.ImplicitSender\r\n \r\nabstract class StatsSampleSpec extends MultiNodeSpec(StatsSampleSpecConfig)\r\n  with WordSpecLike with Matchers with BeforeAndAfterAll\r\n  with ImplicitSender {\r\n \r\n  import StatsSampleSpecConfig._\r\n \r\n  override def initialParticipants = roles.size\r\n \r\n  override def beforeAll() = multiNodeSpecBeforeAll()\r\n \r\n  override def afterAll() = multiNodeSpecAfterAll()\r\nMost of this can of course be extracted to a separate trait to avoid repeating this in all your tests.\r\n\r\nTypically you begin your test by starting up the cluster and let the members join, and create some actors. That can be done like this:\r\n\r\n\"illustrate how to startup cluster\" in within(15 seconds) {\r\n  Cluster(system).subscribe(testActor, classOf[MemberUp])\r\n  expectMsgClass(classOf[CurrentClusterState])\r\n \r\n  val firstAddress = node(first).address\r\n  val secondAddress = node(second).address\r\n  val thirdAddress = node(third).address\r\n \r\n  Cluster(system) join firstAddress\r\n \r\n  system.actorOf(Props[StatsWorker], \"statsWorker\")\r\n  system.actorOf(Props[StatsService], \"statsService\")\r\n \r\n  receiveN(3).collect { case MemberUp(m) => m.address }.toSet should be(\r\n    Set(firstAddress, secondAddress, thirdAddress))\r\n \r\n  Cluster(system).unsubscribe(testActor)\r\n \r\n  testConductor.enter(\"all-up\")\r\n}\r\nFrom the test you interact with the cluster using the Cluster extension, e.g. join.\r\n\r\nCluster(system) join firstAddress\r\nNotice how the testActor from testkit is added as subscriber to cluster changes and then waiting for certain events, such as in this case all members becoming 'Up'.\r\n\r\nThe above code was running for all roles (JVMs). runOn is a convenient utility to declare that a certain block of code should only run for a specific role.\r\n\r\n\"show usage of the statsService from one node\" in within(15 seconds) {\r\n  runOn(second) {\r\n    assertServiceOk()\r\n  }\r\n \r\n  testConductor.enter(\"done-2\")\r\n}\r\n \r\ndef assertServiceOk(): Unit = {\r\n  val service = system.actorSelection(node(third) / \"user\" / \"statsService\")\r\n  // eventually the service should be ok,\r\n  // first attempts might fail because worker actors not started yet\r\n  awaitAssert {\r\n    service ! StatsJob(\"this is the text that will be analyzed\")\r\n    expectMsgType[StatsResult](1.second).meanWordLength should be(\r\n      3.875 +- 0.001)\r\n  }\r\n \r\n}\r\nOnce again we take advantage of the facilities in testkit to verify expected behavior. Here using testActor as sender (via ImplicitSender) and verifing the reply with expectMsgPF.\r\n\r\nIn the above code you can see node(third), which is useful facility to get the root actor reference of the actor system for a specific role. This can also be used to grab the akka.actor.Address of that node.\r\n\r\nval firstAddress = node(first).address\r\nval secondAddress = node(second).address\r\nval thirdAddress = node(third).address\r\nJMX\r\nInformation and management of the cluster is available as JMX MBeans with the root name akka.Cluster. The JMX information can be displayed with an ordinary JMX console such as JConsole or JVisualVM.\r\n\r\nFrom JMX you can:\r\n\r\nsee what members that are part of the cluster\r\nsee status of this node\r\nsee roles of each member\r\njoin this node to another node in cluster\r\nmark any node in the cluster as down\r\ntell any node in the cluster to leave\r\nMember nodes are identified by their address, in format akka.<protocol>://<actor-system-name>@<hostname>:<port>.\r\n\r\nCommand Line Management\r\nThe cluster can be managed with the script bin/akka-cluster provided in the Akka distribution.\r\n\r\nRun it without parameters to see instructions about how to use the script:\r\n\r\nUsage: bin/akka-cluster <node-hostname> <jmx-port> <command> ...\r\n \r\nSupported commands are:\r\n           join <node-url> - Sends request a JOIN node with the specified URL\r\n          leave <node-url> - Sends a request for node with URL to LEAVE the cluster\r\n           down <node-url> - Sends a request for marking node with URL as DOWN\r\n             member-status - Asks the member node for its current status\r\n                   members - Asks the cluster for addresses of current members\r\n               unreachable - Asks the cluster for addresses of unreachable members\r\n            cluster-status - Asks the cluster for its current status (member ring,\r\n                             unavailable nodes, meta data etc.)\r\n                    leader - Asks the cluster who the current leader is\r\n              is-singleton - Checks if the cluster is a singleton cluster (single\r\n                             node cluster)\r\n              is-available - Checks if the member node is available\r\nWhere the <node-url> should be on the format of\r\n  'akka.<protocol>://<actor-system-name>@<hostname>:<port>'\r\n \r\nExamples: bin/akka-cluster localhost 9999 is-available\r\n          bin/akka-cluster localhost 9999 join akka.tcp://MySystem@darkstar:2552\r\n          bin/akka-cluster localhost 9999 cluster-status\r\nTo be able to use the script you must enable remote monitoring and management when starting the JVMs of the cluster nodes, as described in Monitoring and Management Using JMX Technology\r\n\r\nExample of system properties to enable remote monitoring and management:\r\n\r\njava -Dcom.sun.management.jmxremote.port=9999 \\\r\n-Dcom.sun.management.jmxremote.authenticate=false \\\r\n-Dcom.sun.management.jmxremote.ssl=false\r\nConfiguration\r\nThere are several configuration properties for the cluster. We refer to the reference configuration for more information.\r\n\r\nCluster Info Logging\r\nYou can silence the logging of cluster events at info level with configuration property:\r\n\r\nakka.cluster.log-info = off\r\nCluster Dispatcher\r\nUnder the hood the cluster extension is implemented with actors and it can be necessary to create a bulkhead for those actors to avoid disturbance from other actors. Especially the heartbeating actors that is used for failure detection can generate false positives if they are not given a chance to run at regular intervals. For this purpose you can define a separate dispatcher to be used for the cluster actors:\r\n\r\nakka.cluster.use-dispatcher = cluster-dispatcher\r\n \r\ncluster-dispatcher {\r\n  type = \"Dispatcher\"\r\n  executor = \"fork-join-executor\"\r\n  fork-join-executor {\r\n    parallelism-min = 2\r\n    parallelism-max = 4\r\n  }\r\n}\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}